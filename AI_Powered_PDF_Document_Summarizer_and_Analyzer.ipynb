{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4KJljJZ61MtZsamKxZ2iK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ragavi203/AI-Powered-PDF-Document-Summarizer-and-Analyzer/blob/main/AI_Powered_PDF_Document_Summarizer_and_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1H4q8ImILnp"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install PyPDF2 transformers nltk spacy\n",
        "\n",
        "import PyPDF2\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy\n",
        "import textwrap\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "class EnhancedPDFAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Initialize AI models\n",
        "        self.bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_file):\n",
        "        \"\"\"Extract text from uploaded PDF file\"\"\"\n",
        "        reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        print(f\"Processing {len(reader.pages)} pages...\")\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        \"\"\"Generate summary using BART model\"\"\"\n",
        "        chunks = [text[i:i+1024] for i in range(0, len(text), 1024)]\n",
        "        summaries = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
        "            try:\n",
        "                summary = self.bart_summarizer(chunk, max_length=130, min_length=30, do_sample=False)\n",
        "                summaries.append(summary[0]['summary_text'])\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Summarization failed for chunk {i+1}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return \" \".join(summaries)\n",
        "\n",
        "    def extract_key_insights(self, text):\n",
        "        \"\"\"Extract key insights using rule-based processing\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        insights = [sent.text for sent in doc.sents if len(sent.text) > 20][:5]\n",
        "        return insights\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment of the text\"\"\"\n",
        "        sentences = sent_tokenize(text[:10000])  # Analyze first 10000 chars for sentiment\n",
        "        sentiments = self.sentiment_analyzer(sentences)\n",
        "        overall_sentiment = max(set(s['label'] for s in sentiments),\n",
        "                              key=lambda x: sum(1 for s in sentiments if s['label'] == x))\n",
        "        return overall_sentiment\n",
        "\n",
        "    def analyze_document(self, text):\n",
        "        \"\"\"Perform comprehensive document analysis\"\"\"\n",
        "        print(\"Generating summary...\")\n",
        "        summary = self.generate_summary(text)\n",
        "\n",
        "        print(\"Extracting key insights...\")\n",
        "        key_insights = self.extract_key_insights(text)\n",
        "\n",
        "        print(\"Analyzing sentiment...\")\n",
        "        sentiment = self.analyze_sentiment(text)\n",
        "\n",
        "        return {\n",
        "            'summary': summary,\n",
        "            'key_insights': key_insights,\n",
        "            'sentiment': sentiment\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to Enhanced PDF Analyzer!\")\n",
        "\n",
        "    analyzer = EnhancedPDFAnalyzer()\n",
        "\n",
        "    print(\"\\nPlease upload your PDF file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        try:\n",
        "            print(f\"\\nAnalyzing {filename}...\")\n",
        "\n",
        "            # Extract text\n",
        "            text = analyzer.extract_text_from_pdf(filename)\n",
        "            print(f\"Extracted {len(text.split())} words from PDF\")\n",
        "\n",
        "            # Analyze document\n",
        "            results = analyzer.analyze_document(text)\n",
        "\n",
        "            # Print results\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"DOCUMENT ANALYSIS RESULTS\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            print(\"\\nSUMMARY:\")\n",
        "            print(textwrap.fill(results['summary'], width=80))\n",
        "\n",
        "            print(\"\\nKEY INSIGHTS:\")\n",
        "            for insight in results['key_insights']:\n",
        "                print(f\"- {insight}\")\n",
        "\n",
        "            print(\"\\nOVERALL SENTIMENT:\")\n",
        "            print(results['sentiment'])\n",
        "\n",
        "            # Save results to file\n",
        "            with open('analysis_results.json', 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, indent=2)\n",
        "            print(\"\\nFull analysis has been saved to 'analysis_results.json'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}